1. 推荐的重构思路
1.1. 设计一个 TunnelConnectionManager（或 TunnelSessionManager）
负责维护 client_id/session_id 到 tx/rx 的映射。
提供线程安全的注册、查找、移除接口。
支持多 client、多 stream、多协议的高效查找。

```
use std::collections::HashMap;
use tokio::sync::{mpsc, Mutex};
use std::sync::Arc;

pub struct TunnelConnectionManager {
    // client_id -> tx
    client_txs: Mutex<HashMap<String, mpsc::Sender<TunnelMessage>>>,
    // session_id -> (client_id, tx)
    session_map: Mutex<HashMap<String, (String, mpsc::Sender<TunnelMessage>)>>,
}

impl TunnelConnectionManager {
    pub fn new() -> Self {
        Self {
            client_txs: Mutex::new(HashMap::new()),
            session_map: Mutex::new(HashMap::new()),
        }
    }

    // 注册新 client
    pub async fn register_client(&self, client_id: String, tx: mpsc::Sender<TunnelMessage>) {
        self.client_txs.lock().await.insert(client_id, tx);
    }

    // 注册新 session
    pub async fn register_session(&self, session_id: String, client_id: String) {
        if let Some(tx) = self.client_txs.lock().await.get(&client_id).cloned() {
            self.session_map.lock().await.insert(session_id, (client_id, tx));
        }
    }

    // 查找目标 tx
    pub async fn get_tx_by_client(&self, client_id: &str) -> Option<mpsc::Sender<TunnelMessage>> {
        self.client_txs.lock().await.get(client_id).cloned()
    }

    pub async fn get_tx_by_session(&self, session_id: &str) -> Option<mpsc::Sender<TunnelMessage>> {
        self.session_map.lock().await.get(session_id).map(|(_, tx)| tx.clone())
    }

    // 移除 client/session
    pub async fn remove_client(&self, client_id: &str) {
        self.client_txs.lock().await.remove(client_id);
        self.session_map.lock().await.retain(|_, (cid, _)| cid != client_id);
    }
    pub async fn remove_session(&self, session_id: &str) {
        self.session_map.lock().await.remove(session_id);
    }
}
```



好的，下面详细分析client 和 server 建立隧道（tunnel）的完整流程，结合你当前的代码结构和关键点。
1. Client 侧建立 tunnel 的流程
1.1. 启动与准备
client 启动后，生成自己的 client_id 和 group_id，并初始化本地的 mpsc channel（tx, rx）。
client 通过 TunnelServiceClient::connect 连接 server，准备好 gRPC 客户端。
1.2. 注册与配置同步
client 先通过 register RPC 主动向 server 注册自己（client_id, group_id）。
client 通过 config_sync RPC 拉取最新的规则和 upstream 配置。
1.3. 建立 gRPC tunnel（proxy）
client 创建 tokio_stream::wrappers::ReceiverStream，将本地 rx 作为 gRPC 的 outbound stream。
client 调用 grpc_client.proxy(Request::new(outbound))，建立 gRPC 双向流（tunnel）。
client 拿到 server 返回的 inbound stream，进入 while let Some(message) = inbound.next().await 循环，持续监听 server 发来的消息。
1.4. 发送心跳、配置同步等
client 启动心跳和配置同步任务，定期通过本地 tx 发送消息到 server（这些消息通过 gRPC tunnel 发送）。
2. Server 侧建立 tunnel 的流程
2.1. gRPC proxy 方法被调用
client 调用 proxy 后，server 的 TunnelServer::proxy 被触发，拿到一个 tonic::Streaming<TunnelMessage>（client -> server）。
server 创建一个本地 mpsc channel (tx, rx)，tx 用于 server -> client，rx 作为 gRPC 返回流。
2.2. 处理 client 消息
server 启动一个 tokio 任务，循环读取 client 发来的消息（while let Some(message) = stream.next().await）。
每当收到 client 的消息时，server 可以拿到 client_id。
2.3. 注册连接
server 在第一次收到 client 的消息时，才会将 client_id、token、tx 注册到本地的连接管理结构（如 connected_clients、token_map）。
server 还会为每个 client 启动一个任务，将本地 mpsc channel 的消息转发到 gRPC 返回流（即 tx.send(Ok(tunnel_msg)).await）。
2.4. 处理 HTTP 请求转发
server HTTP 入口收到请求时，会根据规则选出 client group，然后遍历 healthy_clients，查找 connected_clients 里是否有活跃的 client 连接（即 mpsc::Sender）。
找到后，通过 mpsc channel 发送 TunnelMessage 到 client，client 处理后再通过 tunnel 返回响应。
3. 关键点与易错点
3.1. client_id 的注册时机
server 只有在收到 client 的第一个消息时，才知道 client_id 并注册连接。
如果 HTTP 请求来的比 client 注册还早，server 端查不到 client 连接。
3.2. mpsc channel 的配对
client 本地的 tx/rx 和 server 端的 rx/tx 通过 gRPC stream 连接起来，形成双向通道。
server 端的 tx 通过 gRPC 返回流发给 client，client 端的 tx 通过 gRPC outbound stream发给 server。
3.3. 连接管理
server 端用 connected_clients（client_id -> mpsc::Sender）和 token_map 管理所有活跃连接。
只有在 client 端 tunnel 连接活跃时，server 端的 mpsc::Sender 才能正常 send。
3.4. 断开与清理
client 断开时，server 端的 stream 结束，相关连接和 token 需要及时清理。


sequenceDiagram
    participant C as Client
    participant S as Server

    C->>S: register(client_id, group_id)
    S->>C: register_response

    C->>S: config_sync(client_id, group_id)
    S->>C: config_sync_response

    C->>S: proxy (gRPC stream, outbound=ReceiverStream)
    S->>C: 返回 gRPC stream (inbound=ReceiverStream)

    loop tunnel
        C->>S: TunnelMessage (心跳/请求/配置)
        S->>C: TunnelMessage (响应/推送)
    end

    Note over S: S 在收到第一个 TunnelMessage 后注册 client_id/tx
    Note over S: S HTTP 入口转发请求时查找 connected_clients


    你的代码现状
client 和 server 的 tunnel 建立流程是标准的 gRPC 双向流模式。
server 端只有在收到 client 的第一个消息时，才注册 client_id -> mpsc::Sender。
HTTP 请求如果在 client 注册前到达，server 端查不到 client 连接，会 502。
连接断开时，server 端会清理相关映射。
6. 建议
如需更快 tunnel 可用性，可在 gRPC 连接建立时就注册一个临时连接，收到 client_id 后再补全映射。
client_id 必须以 client 上报为准，不能 server 自己生成。
所有消息转发都应通过连接管理器查找活跃连接，不要依赖其他副本。