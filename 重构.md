1. 推荐的重构思路
1.1. 设计一个 TunnelConnectionManager（或 TunnelSessionManager）
负责维护 client_id/session_id 到 tx/rx 的映射。
提供线程安全的注册、查找、移除接口。
支持多 client、多 stream、多协议的高效查找。

```
use std::collections::HashMap;
use tokio::sync::{mpsc, Mutex};
use std::sync::Arc;

pub struct TunnelConnectionManager {
    // client_id -> tx
    client_txs: Mutex<HashMap<String, mpsc::Sender<TunnelMessage>>>,
    // session_id -> (client_id, tx)
    session_map: Mutex<HashMap<String, (String, mpsc::Sender<TunnelMessage>)>>,
}

impl TunnelConnectionManager {
    pub fn new() -> Self {
        Self {
            client_txs: Mutex::new(HashMap::new()),
            session_map: Mutex::new(HashMap::new()),
        }
    }

    // 注册新 client
    pub async fn register_client(&self, client_id: String, tx: mpsc::Sender<TunnelMessage>) {
        self.client_txs.lock().await.insert(client_id, tx);
    }

    // 注册新 session
    pub async fn register_session(&self, session_id: String, client_id: String) {
        if let Some(tx) = self.client_txs.lock().await.get(&client_id).cloned() {
            self.session_map.lock().await.insert(session_id, (client_id, tx));
        }
    }

    // 查找目标 tx
    pub async fn get_tx_by_client(&self, client_id: &str) -> Option<mpsc::Sender<TunnelMessage>> {
        self.client_txs.lock().await.get(client_id).cloned()
    }

    pub async fn get_tx_by_session(&self, session_id: &str) -> Option<mpsc::Sender<TunnelMessage>> {
        self.session_map.lock().await.get(session_id).map(|(_, tx)| tx.clone())
    }

    // 移除 client/session
    pub async fn remove_client(&self, client_id: &str) {
        self.client_txs.lock().await.remove(client_id);
        self.session_map.lock().await.retain(|_, (cid, _)| cid != client_id);
    }
    pub async fn remove_session(&self, session_id: &str) {
        self.session_map.lock().await.remove(session_id);
    }
}
```



好的，下面详细分析client 和 server 建立隧道（tunnel）的完整流程，结合你当前的代码结构和关键点。
1. Client 侧建立 tunnel 的流程
1.1. 启动与准备
client 启动后，生成自己的 client_id 和 group_id，并初始化本地的 mpsc channel（tx, rx）。
client 通过 TunnelServiceClient::connect 连接 server，准备好 gRPC 客户端。
1.2. 注册与配置同步
client 先通过 register RPC 主动向 server 注册自己（client_id, group_id）。
client 通过 config_sync RPC 拉取最新的规则和 upstream 配置。
1.3. 建立 gRPC tunnel（proxy）
client 创建 tokio_stream::wrappers::ReceiverStream，将本地 rx 作为 gRPC 的 outbound stream。
client 调用 grpc_client.proxy(Request::new(outbound))，建立 gRPC 双向流（tunnel）。
client 拿到 server 返回的 inbound stream，进入 while let Some(message) = inbound.next().await 循环，持续监听 server 发来的消息。
1.4. 发送心跳、配置同步等
client 启动心跳和配置同步任务，定期通过本地 tx 发送消息到 server（这些消息通过 gRPC tunnel 发送）。
2. Server 侧建立 tunnel 的流程
2.1. gRPC proxy 方法被调用
client 调用 proxy 后，server 的 TunnelServer::proxy 被触发，拿到一个 tonic::Streaming<TunnelMessage>（client -> server）。
server 创建一个本地 mpsc channel (tx, rx)，tx 用于 server -> client，rx 作为 gRPC 返回流。
2.2. 处理 client 消息
server 启动一个 tokio 任务，循环读取 client 发来的消息（while let Some(message) = stream.next().await）。
每当收到 client 的消息时，server 可以拿到 client_id。
2.3. 注册连接
server 在第一次收到 client 的消息时，才会将 client_id、token、tx 注册到本地的连接管理结构（如 connected_clients、token_map）。
server 还会为每个 client 启动一个任务，将本地 mpsc channel 的消息转发到 gRPC 返回流（即 tx.send(Ok(tunnel_msg)).await）。
2.4. 处理 HTTP 请求转发
server HTTP 入口收到请求时，会根据规则选出 client group，然后遍历 healthy_clients，查找 connected_clients 里是否有活跃的 client 连接（即 mpsc::Sender）。
找到后，通过 mpsc channel 发送 TunnelMessage 到 client，client 处理后再通过 tunnel 返回响应。
3. 关键点与易错点
3.1. client_id 的注册时机
server 只有在收到 client 的第一个消息时，才知道 client_id 并注册连接。
如果 HTTP 请求来的比 client 注册还早，server 端查不到 client 连接。
3.2. mpsc channel 的配对
client 本地的 tx/rx 和 server 端的 rx/tx 通过 gRPC stream 连接起来，形成双向通道。
server 端的 tx 通过 gRPC 返回流发给 client，client 端的 tx 通过 gRPC outbound stream发给 server。
3.3. 连接管理
server 端用 connected_clients（client_id -> mpsc::Sender）和 token_map 管理所有活跃连接。
只有在 client 端 tunnel 连接活跃时，server 端的 mpsc::Sender 才能正常 send。
3.4. 断开与清理
client 断开时，server 端的 stream 结束，相关连接和 token 需要及时清理。


sequenceDiagram
    participant C as Client
    participant S as Server

    C->>S: register(client_id, group_id)
    S->>C: register_response

    C->>S: config_sync(client_id, group_id)
    S->>C: config_sync_response

    C->>S: proxy (gRPC stream, outbound=ReceiverStream)
    S->>C: 返回 gRPC stream (inbound=ReceiverStream)

    loop tunnel
        C->>S: TunnelMessage (心跳/请求/配置)
        S->>C: TunnelMessage (响应/推送)
    end

    Note over S: S 在收到第一个 TunnelMessage 后注册 client_id/tx
    Note over S: S HTTP 入口转发请求时查找 connected_clients


    你的代码现状
client 和 server 的 tunnel 建立流程是标准的 gRPC 双向流模式。
server 端只有在收到 client 的第一个消息时，才注册 client_id -> mpsc::Sender。
HTTP 请求如果在 client 注册前到达，server 端查不到 client 连接，会 502。
连接断开时，server 端会清理相关映射。
6. 建议
如需更快 tunnel 可用性，可在 gRPC 连接建立时就注册一个临时连接，收到 client_id 后再补全映射。
client_id 必须以 client 上报为准，不能 server 自己生成。
所有消息转发都应通过连接管理器查找活跃连接，不要依赖其他副本。



===

Tunnel 多协议多路复用架构设计
1. 概述
本文档描述了基于 gRPC 的隧道系统，支持 HTTP、gRPC、WebSocket 等多协议的多路复用架构设计。通过统一的 envelope 封装和 stream 管理，实现高效的多协议隧道通信。
2. 核心设计原则
2.1 协议封装
使用 TunnelEnvelope 作为统一的消息封装
支持多种协议类型：HTTP、gRPC、WebSocket
通过 session_id 实现多路复用
通过 stream_id 管理物理连接
2.2 连接管理
单一 gRPC stream 支持多协议多会话
通过 TunnelConnectionManager 统一管理 stream 生命周期
支持自动重连和故障恢复
3. Proto 设计
3.1 TunnelEnvelope 结构

```
syntax = "proto3";

package tunnel;

message TunnelEnvelope {
  string protocol = 1;      // "http", "grpc", "websocket", ...
  string session_id = 2;    // 用于流式/长连接/会话
  string stream_id = 3;     // 物理/逻辑 stream 的唯一标识
  oneof payload {
    HttpRequest http_request = 5;
    HttpResponse http_response = 6;
    GrpcFrame grpc_frame = 7;
    WebSocketFrame ws_frame = 8;
    TunnelError error = 9;
  }
}

// HTTP 协议
message HttpRequest {
  string method = 1;
  string url = 2;
  string host = 3;
  string path = 4;
  string query = 5;
  map<string, string> headers = 6;
  bytes body = 7;
  string original_dst = 8;
}

message HttpResponse {
  int32 status_code = 1;
  map<string, string> headers = 2;
  bytes body = 3;
}

// gRPC 协议（支持所有模式）
message GrpcFrame {
  enum FrameType {
    DATA = 0;         // 普通数据帧
    METADATA = 1;     // metadata（header）
    TRAILER = 2;      // trailer
    ERROR = 3;        // 错误帧
    CLOSE = 4;        // 主动关闭流
  }
  FrameType frame_type = 1;
  string service = 2;         // 服务名
  string method = 3;          // 方法名
  map<string, string> metadata = 4; // metadata/header/trailer
  bytes data = 5;             // 数据帧内容
  int32 status_code = 6;      // gRPC status code（仅 ERROR/TRAILER 用）
  string status_message = 7;  // 错误描述
}

// WebSocket 协议
message WebSocketFrame {
  enum OpCode {
    CONTINUATION = 0;
    TEXT = 1;
    BINARY = 2;
    CLOSE = 8;
    PING = 9;
    PONG = 10;
  }
  OpCode opcode = 1;
  bytes payload = 2;
  bool fin = 3; // 是否为消息最后一帧
  uint64 seq = 4; // 可选：帧序号，便于重组
  map<string, string> headers = 5; // 可选：握手/扩展用
}

// 通用错误
message TunnelError {
  int32 code = 1;
  string message = 2;
  string detail = 3;
}
```

3.2 字段说明
字段	类型	说明
protocol	string	协议类型标识
session_id	string	会话/连接/流内唯一标识
stream_id	string	物理/逻辑 stream 的唯一标识
stream_name	string	stream 的语义化名称
4. 架构组件设计
4.1 TunnelConnectionManager（统一对象）

```
use std::collections::HashMap;
use tokio::sync::{mpsc, Mutex};
use std::sync::Arc;
use dashmap::DashMap;

pub struct StreamInfo {
    pub stream_id: String,
    pub stream_name: String,
    pub tx: mpsc::Sender<TunnelMessage>,
    pub client_id: Option<String>, // server 端使用
    pub group_id: Option<String>,  // server 端使用
}

pub struct TunnelConnectionManager {
    // stream_id -> StreamInfo
    streams: DashMap<String, StreamInfo>,
}

impl TunnelConnectionManager {
    pub fn new() -> Self {
        Self {
            streams: DashMap::new(),
        }
    }

    // 注册新 stream
    pub async fn register_stream(&self, stream_info: StreamInfo) {
        self.streams.insert(stream_info.stream_id.clone(), stream_info);
    }

    // 根据 stream_id 获取 tx
    pub async fn get_tx_by_stream_id(&self, stream_id: &str) -> Option<mpsc::Sender<TunnelMessage>> {
        self.streams.get(stream_id).map(|info| info.tx.clone())
    }

    // 移除 stream
    pub async fn remove_stream(&self, stream_id: &str) {
        self.streams.remove(stream_id);
    }

    // 获取所有 stream
    pub fn get_all_streams(&self) -> Vec<StreamInfo> {
        self.streams.iter().map(|entry| entry.value().clone()).collect()
    }
}
```

4.2 ServerTunnelManager（Server 端特有）
```
pub struct ServerTunnelManager {
    connection_manager: Arc<TunnelConnectionManager>,
    // client_id -> stream_id
    client_streams: DashMap<String, String>,
    // client_group -> Vec<client_id>
    client_groups: DashMap<String, Vec<String>>,
    // client_id -> ClientInfo
    client_infos: DashMap<String, ClientInfo>,
}

#[derive(Clone)]
pub struct ClientInfo {
    pub client_id: String,
    pub group_id: String,
    pub stream_id: String,
    pub last_heartbeat: u64,
    pub is_healthy: bool,
}

impl ServerTunnelManager {
    pub fn new(connection_manager: Arc<TunnelConnectionManager>) -> Self {
        Self {
            connection_manager,
            client_streams: DashMap::new(),
            client_groups: DashMap::new(),
            client_infos: DashMap::new(),
        }
    }

    // 注册新 client
    pub async fn register_client(&self, client_id: String, stream_id: String, group_id: String) {
        self.client_streams.insert(client_id.clone(), stream_id.clone());
        
        // 添加到 group
        self.client_groups.entry(group_id.clone()).or_insert_with(Vec::new).push(client_id.clone());
        
        // 更新 client info
        let client_info = ClientInfo {
            client_id: client_id.clone(),
            group_id,
            stream_id,
            last_heartbeat: SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs(),
            is_healthy: true,
        };
        self.client_infos.insert(client_id, client_info);
    }

    // 根据 client_id 获取 tx
    pub async fn get_tx_by_client(&self, client_id: &str) -> Option<mpsc::Sender<TunnelMessage>> {
        if let Some(stream_id) = self.client_streams.get(client_id) {
            self.connection_manager.get_tx_by_stream_id(&stream_id).await
        } else {
            None
        }
    }

    // 根据 group 选择 healthy client
    pub async fn select_client_in_group(&self, group_id: &str) -> Option<String> {
        if let Some(clients) = self.client_groups.get(group_id) {
            for client_id in clients.iter() {
                if let Some(client_info) = self.client_infos.get(client_id) {
                    if client_info.is_healthy {
                        return Some(client_id.clone());
                    }
                }
            }
        }
        None
    }

    // 更新 client 心跳
    pub async fn update_client_heartbeat(&self, client_id: &str) {
        if let Some(mut client_info) = self.client_infos.get_mut(client_id) {
            client_info.last_heartbeat = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();
        }
    }

    // 移除 client
    pub async fn remove_client(&self, client_id: &str) {
        if let Some(client_info) = self.client_infos.get(client_id) {
            // 从 group 中移除
            if let Some(mut clients) = self.client_groups.get_mut(&client_info.group_id) {
                clients.retain(|id| id != client_id);
            }
        }
        
        self.client_streams.remove(client_id);
        self.client_infos.remove(client_id);
    }
}
```

```
5. 协议识别与分发
5.1 Handler 设计
```
pub struct TunnelHandler {
    server_manager: Arc<ServerTunnelManager>, // server 端
    connection_manager: Arc<TunnelConnectionManager>, // client 端
}

impl TunnelHandler {
    // 处理接收到的 TunnelEnvelope
    pub async fn handle_envelope(&self, envelope: TunnelEnvelope) -> Result<(), Box<dyn std::error::Error>> {
        match envelope.protocol.as_str() {
            "http" => self.handle_http(envelope).await,
            "grpc" => self.handle_grpc(envelope).await,
            "websocket" => self.handle_websocket(envelope).await,
            _ => Err("Unknown protocol".into()),
        }
    }

    // HTTP 处理
    async fn handle_http(&self, envelope: TunnelEnvelope) -> Result<(), Box<dyn std::error::Error>> {
        match &envelope.payload {
            Some(TunnelEnvelopePayload::HttpRequest(req)) => {
                // 根据规则路由到目标 client
                if let Some(target_client) = self.route_http_request(req).await {
                    if let Some(tx) = self.server_manager.get_tx_by_client(&target_client).await {
                        tx.send(envelope).await?;
                    }
                }
            }
            Some(TunnelEnvelopePayload::HttpResponse(resp)) => {
                // 处理 HTTP 响应
                self.handle_http_response(envelope).await?;
            }
            _ => {}
        }
        Ok(())
    }

    // gRPC 处理
    async fn handle_grpc(&self, envelope: TunnelEnvelope) -> Result<(), Box<dyn std::error::Error>> {
        match &envelope.payload {
            Some(TunnelEnvelopePayload::GrpcFrame(frame)) => {
                // 根据 service/method 路由
                if let Some(target_client) = self.route_grpc_request(frame).await {
                    if let Some(tx) = self.server_manager.get_tx_by_client(&target_client).await {
                        tx.send(envelope).await?;
                    }
                }
            }
            _ => {}
        }
        Ok(())
    }

    // WebSocket 处理
    async fn handle_websocket(&self, envelope: TunnelEnvelope) -> Result<(), Box<dyn std::error::Error>> {
        match &envelope.payload {
            Some(TunnelEnvelopePayload::WsFrame(frame)) => {
                // 根据 session_id 路由
                if let Some(target_client) = self.route_websocket_frame(&envelope.session_id).await {
                    if let Some(tx) = self.server_manager.get_tx_by_client(&target_client).await {
                        tx.send(envelope).await?;
                    }
                }
            }
            _ => {}
        }
        Ok(())
    }
}
```
6. 连接生命周期管理
6.1 自动重连机制

```
pub struct TunnelConnection {
    stream_id: String,
    stream_name: String,
    tx: mpsc::Sender<TunnelMessage>,
    rx: mpsc::Receiver<TunnelMessage>,
    manager: Arc<TunnelConnectionManager>,
}

impl TunnelConnection {
    // 启动连接监听
    pub async fn start(&mut self) {
        while let Some(msg) = self.rx.recv().await {
            // 处理消息
            if let Err(e) = self.handle_message(msg).await {
                // 连接异常，触发重连
                self.reconnect().await;
                break;
            }
        }
    }

    // 重连逻辑
    async fn reconnect(&self) {
        let mut backoff = ExponentialBackoff::default();
        
        loop {
            match self.establish_connection().await {
                Ok((new_tx, new_rx)) => {
                    // 重新注册到 manager
                    let stream_info = StreamInfo {
                        stream_id: self.stream_id.clone(),
                        stream_name: self.stream_name.clone(),
                        tx: new_tx,
                        client_id: None,
                        group_id: None,
                    };
                    self.manager.register_stream(stream_info).await;
                    break;
                }
                Err(_) => {
                    if let Some(delay) = backoff.next_backoff() {
                        tokio::time::sleep(delay).await;
                    } else {
                        break;
                    }
                }
            }
        }
    }
}
```
7. 总结
采用统一的 TunnelConnectionManager 对象管理所有 stream 的 tx/rx，便于多协议多 stream 的高效路由和生命周期管理。
server 端通过额外的映射（如 client_id → stream_id、client_group → Vec<client_id>）实现对多 client group、多 client 的灵活管理和负载均衡。
proto 层通过 stream_id、stream_name 字段为未来多 stream/多连接/多协议扩展打下基础。
handler 只需通过 manager 查找目标 tx，发送消息即可，极大简化了业务逻辑。
自动重连、健康检查等高可用机制可以在 manager/connection 层统一实现，业务层无感知。



stream name/id 与 channel/stream 的关联
你在管理层（如 TunnelConnectionManager）里，维护了 stream name/id 到 channel/stream/tx 的映射关系。

```
  // Rust伪代码
  struct StreamInfo {
      stream_name: String,
      session_id: String,
      channel_id: String, // 或 channel/connection 的唯一标识
      tx: mpsc::Sender<TunnelMessage>,
      // 其他元数据
  }
  // manager: stream_name/id -> StreamInfo
```
2. channel/stream 断开时的感知
gRPC/tonic：当 channel 断开或 stream 关闭时，相关的 rx/tx 会返回错误（如 Err(Status)），你可以在 handler 或管理器里捕获到。
检测方式：
读写 stream 时遇到 Err(Status)，说明该 stream/channel 已不可用。
tonic 的 channel 支持健康检查（如 .ready().await），可以主动检测 channel 状态。
3. 自动 cancel 和重建流程
步骤一：感知断开
在每个 stream 的 handler/task 里，监听 rx/tx 的错误返回。
一旦发现错误（如 Status::unavailable()、Status::cancelled()），立即触发清理和重建逻辑。
步骤二：清理关联
在 TunnelConnectionManager 里，将该 channel/stream 关联的所有 stream name/id 标记为“不可用”或直接移除。
通知上层 handler 取消相关 session。
步骤三：自动重建
启动重连/重建逻辑（可用 backoff、定时重试等）。
新建 channel/stream 后，重新注册 stream name/id 到新的 tx/rx。
恢复业务流量。
4. Rust 伪代码示例
// 1. 维护映射
struct TunnelConnectionManager {
    // stream_name -> StreamInfo
    streams: DashMap<String, StreamInfo>,
    // channel_id -> Vec<stream_name>
    channel_streams: DashMap<String, Vec<String>>,
}

// 2. 断开时清理
fn on_channel_closed(&self, channel_id: &str) {
    if let Some(stream_names) = self.channel_streams.remove(channel_id) {
        for stream_name in stream_names {
            self.streams.remove(&stream_name);
            // 通知 handler/session cancel
        }
    }
}

// 3. handler 里检测
async fn handle_stream(stream_info: StreamInfo, manager: Arc<TunnelConnectionManager>) {
    while let Some(msg) = stream_info.rx.recv().await {
        // ...
    }
    // rx关闭，说明stream/channel挂了
    manager.on_channel_closed(&stream_info.channel_id);
    // 触发重建
}

自动重建的最佳实践
重连/重建：用 exponential backoff，避免频繁重试。
幂等注册：新建 stream/channel 后，重新注册 stream name/id 到 manager。
业务透明：上层 handler 只需通过 manager 获取可用的 tx，manager 自动屏蔽重建细节。
6. 总结
通过 manager 维护 stream name/id 到 channel/stream/tx 的映射。
channel/stream 断开时，捕获异常，自动清理和 cancel 相关 session。
自动重建 channel/stream 并重新注册映射，业务层无感知。
这样可以实现高可用、自动恢复的 tunnel 通信。


====

1. 性能优化
Server 端
连接管理与负载均衡

ManagedClientRegistry 已支持 group 内健康 client 选择，但可以进一步优化为支持多种负载均衡策略（如随机、最少连接等），并支持动态切换。
对于高并发 tunnel 消息，可以考虑将 mpsc channel buffer 调大，减少因 channel 满导致的 await 阻塞。
规则匹配优化

RulesEngine 规则匹配目前为顺序遍历，规则多时可用 Trie/HashMap 优化 host/path/service 的查找。
支持规则热更新时，采用原子替换（Arc<RulesEngine>），避免锁粒度过大。
异步与并发

HTTP/gRPC 入口处理已用 tokio 并发，注意避免 handler 内部阻塞（如同步 IO）。
对于 pending_requests、connected_clients 等全局状态，优先用 DashMap 或 RwLock，减少锁冲突。
健康检查与自动剔除

get_healthy_streams_in_group 支持超时剔除，建议增加定期主动健康检查（如心跳丢失主动关闭连接）。
Client 端
本地请求分发

ClientHttpEntryTarget 负责本地 HTTP 入口，建议将 HTTP/gRPC handler 逻辑抽象为 trait，便于多协议扩展和复用。
支持本地 fallback（如 tunnel 不可用时本地直连），提升可用性。
tunnel 自动重连

已实现 exponential backoff，建议重连时支持幂等注册和资源自动清理，避免内存泄漏。
配置同步与热更新

client group 规则支持动态下发，建议用 watch channel 或 notify 机制，配置变更时自动刷新本地 matcher。
2. 架构设计与可复用性
分层与职责清晰
连接管理层

统一用 TunnelConnectionManager/ServerTunnelManager 管理所有连接、映射、心跳，所有消息转发都通过 manager 查找活跃连接，避免副本和状态不一致。
连接断开时自动清理所有相关 session/stream，支持自动重建。
规则引擎层

RulesEngine 负责所有规则匹配，建议将规则结构体、匹配算法、优先级等抽象为独立模块，便于 server/client 复用。
支持多协议（HTTP/gRPC）统一规则表达和匹配接口。
入口处理层

HTTP/gRPC 入口 handler 统一实现 ProxyTarget trait，便于 server/client 侧 handler 逻辑解耦和复用。
支持 handler 插件化（如 header 注入、认证、限流等）。
配置与同步层

server 作为配置中心，client 通过 tunnel 拉取配置，建议配置结构体与序列化/反序列化逻辑独立，便于热更新和多 client group 支持。
代码复用建议
将 tunnel 协议、消息、通用 handler、规则引擎等抽象到 tunnel-lib，server/client 只关注自身业务逻辑。
入口 handler、规则匹配、连接管理等都可 trait 化，便于 mock 和单元测试。
支持多 group、多 upstream、多协议扩展时，优先用 trait + 泛型，减少重复代码。
3. 简洁性与分工
server 只负责统一入口、规则分发、连接管理和配置中心，client 只负责本地监听、tunnel 连接和本地规则分发。
规则、upstream、client group、连接状态等都用结构体清晰表达，避免隐式状态。
入口 handler、tunnel handler、规则引擎、连接管理等职责单一，便于维护和扩展。
参考实现与文档
你的 README.md 和 重构.md 已有详细架构说明，建议将关键设计（如 manager/engine/handler 分层、连接生命周期管理、规则热更新流程）补充到代码注释和模块文档中，便于团队协作和后续维护。
如需具体代码重构建议或模块抽象示例，可进一步指定关注点。