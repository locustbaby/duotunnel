Tunnel 多协议多路复用架构设计
1. 概述
本文档描述了基于 gRPC 的隧道系统，支持 HTTP、gRPC、WebSocket 等多协议的多路复用架构设计。通过统一的 envelope 封装和 stream 管理，实现高效的多协议隧道通信。
2. 核心设计原则
2.1 协议封装
使用 TunnelEnvelope 作为统一的消息封装
支持多种协议类型：HTTP、gRPC、WebSocket
通过 session_id 实现多路复用
通过 stream_id 管理物理连接
2.2 连接管理
单一 gRPC stream 支持多协议多会话
通过 TunnelConnectionManager 统一管理 stream 生命周期
支持自动重连和故障恢复
3. Proto 设计
3.1 TunnelEnvelope 结构

```
syntax = "proto3";

package tunnel;

message TunnelEnvelope {
  string protocol = 1;      // "http", "grpc", "websocket", ...
  string session_id = 2;    // 用于流式/长连接/会话
  string stream_id = 3;     // 物理/逻辑 stream 的唯一标识
  oneof payload {
    HttpRequest http_request = 5;
    HttpResponse http_response = 6;
    GrpcFrame grpc_frame = 7;
    WebSocketFrame ws_frame = 8;
    TunnelError error = 9;
  }
}

// HTTP 协议
message HttpRequest {
  string method = 1;
  string url = 2;
  string host = 3;
  string path = 4;
  string query = 5;
  map<string, string> headers = 6;
  bytes body = 7;
  string original_dst = 8;
}

message HttpResponse {
  int32 status_code = 1;
  map<string, string> headers = 2;
  bytes body = 3;
}

// gRPC 协议（支持所有模式）
message GrpcFrame {
  enum FrameType {
    DATA = 0;         // 普通数据帧
    METADATA = 1;     // metadata（header）
    TRAILER = 2;      // trailer
    ERROR = 3;        // 错误帧
    CLOSE = 4;        // 主动关闭流
  }
  FrameType frame_type = 1;
  string service = 2;         // 服务名
  string method = 3;          // 方法名
  map<string, string> metadata = 4; // metadata/header/trailer
  bytes data = 5;             // 数据帧内容
  int32 status_code = 6;      // gRPC status code（仅 ERROR/TRAILER 用）
  string status_message = 7;  // 错误描述
}

// WebSocket 协议
message WebSocketFrame {
  enum OpCode {
    CONTINUATION = 0;
    TEXT = 1;
    BINARY = 2;
    CLOSE = 8;
    PING = 9;
    PONG = 10;
  }
  OpCode opcode = 1;
  bytes payload = 2;
  bool fin = 3; // 是否为消息最后一帧
  uint64 seq = 4; // 可选：帧序号，便于重组
  map<string, string> headers = 5; // 可选：握手/扩展用
}

// 通用错误
message TunnelError {
  int32 code = 1;
  string message = 2;
  string detail = 3;
}
```

3.2 字段说明
字段	类型	说明
protocol	string	协议类型标识
session_id	string	会话/连接/流内唯一标识
stream_id	string	物理/逻辑 stream 的唯一标识
stream_name	string	stream 的语义化名称
4. 架构组件设计
4.1 TunnelConnectionManager（统一对象）

```
use std::collections::HashMap;
use tokio::sync::{mpsc, Mutex};
use std::sync::Arc;
use dashmap::DashMap;

pub struct StreamInfo {
    pub stream_id: String,
    pub stream_name: String,
    pub tx: mpsc::Sender<TunnelMessage>,
    pub client_id: Option<String>, // server 端使用
    pub group_id: Option<String>,  // server 端使用
}

pub struct TunnelConnectionManager {
    // stream_id -> StreamInfo
    streams: DashMap<String, StreamInfo>,
}

impl TunnelConnectionManager {
    pub fn new() -> Self {
        Self {
            streams: DashMap::new(),
        }
    }

    // 注册新 stream
    pub async fn register_stream(&self, stream_info: StreamInfo) {
        self.streams.insert(stream_info.stream_id.clone(), stream_info);
    }

    // 根据 stream_id 获取 tx
    pub async fn get_tx_by_stream_id(&self, stream_id: &str) -> Option<mpsc::Sender<TunnelMessage>> {
        self.streams.get(stream_id).map(|info| info.tx.clone())
    }

    // 移除 stream
    pub async fn remove_stream(&self, stream_id: &str) {
        self.streams.remove(stream_id);
    }

    // 获取所有 stream
    pub fn get_all_streams(&self) -> Vec<StreamInfo> {
        self.streams.iter().map(|entry| entry.value().clone()).collect()
    }
}
```

4.2 ServerTunnelManager（Server 端特有）
```
pub struct ServerTunnelManager {
    connection_manager: Arc<TunnelConnectionManager>,
    // client_id -> stream_id
    client_streams: DashMap<String, String>,
    // client_group -> Vec<client_id>
    client_groups: DashMap<String, Vec<String>>,
    // client_id -> ClientInfo
    client_infos: DashMap<String, ClientInfo>,
}

#[derive(Clone)]
pub struct ClientInfo {
    pub client_id: String,
    pub group_id: String,
    pub stream_id: String,
    pub last_heartbeat: u64,
    pub is_healthy: bool,
}

impl ServerTunnelManager {
    pub fn new(connection_manager: Arc<TunnelConnectionManager>) -> Self {
        Self {
            connection_manager,
            client_streams: DashMap::new(),
            client_groups: DashMap::new(),
            client_infos: DashMap::new(),
        }
    }

    // 注册新 client
    pub async fn register_client(&self, client_id: String, stream_id: String, group_id: String) {
        self.client_streams.insert(client_id.clone(), stream_id.clone());
        
        // 添加到 group
        self.client_groups.entry(group_id.clone()).or_insert_with(Vec::new).push(client_id.clone());
        
        // 更新 client info
        let client_info = ClientInfo {
            client_id: client_id.clone(),
            group_id,
            stream_id,
            last_heartbeat: SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs(),
            is_healthy: true,
        };
        self.client_infos.insert(client_id, client_info);
    }

    // 根据 client_id 获取 tx
    pub async fn get_tx_by_client(&self, client_id: &str) -> Option<mpsc::Sender<TunnelMessage>> {
        if let Some(stream_id) = self.client_streams.get(client_id) {
            self.connection_manager.get_tx_by_stream_id(&stream_id).await
        } else {
            None
        }
    }

    // 根据 group 选择 healthy client
    pub async fn select_client_in_group(&self, group_id: &str) -> Option<String> {
        if let Some(clients) = self.client_groups.get(group_id) {
            for client_id in clients.iter() {
                if let Some(client_info) = self.client_infos.get(client_id) {
                    if client_info.is_healthy {
                        return Some(client_id.clone());
                    }
                }
            }
        }
        None
    }

    // 更新 client 心跳
    pub async fn update_client_heartbeat(&self, client_id: &str) {
        if let Some(mut client_info) = self.client_infos.get_mut(client_id) {
            client_info.last_heartbeat = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();
        }
    }

    // 移除 client
    pub async fn remove_client(&self, client_id: &str) {
        if let Some(client_info) = self.client_infos.get(client_id) {
            // 从 group 中移除
            if let Some(mut clients) = self.client_groups.get_mut(&client_info.group_id) {
                clients.retain(|id| id != client_id);
            }
        }
        
        self.client_streams.remove(client_id);
        self.client_infos.remove(client_id);
    }
}
```

```
5. 协议识别与分发
5.1 Handler 设计
```
pub struct TunnelHandler {
    server_manager: Arc<ServerTunnelManager>, // server 端
    connection_manager: Arc<TunnelConnectionManager>, // client 端
}

impl TunnelHandler {
    // 处理接收到的 TunnelEnvelope
    pub async fn handle_envelope(&self, envelope: TunnelEnvelope) -> Result<(), Box<dyn std::error::Error>> {
        match envelope.protocol.as_str() {
            "http" => self.handle_http(envelope).await,
            "grpc" => self.handle_grpc(envelope).await,
            "websocket" => self.handle_websocket(envelope).await,
            _ => Err("Unknown protocol".into()),
        }
    }

    // HTTP 处理
    async fn handle_http(&self, envelope: TunnelEnvelope) -> Result<(), Box<dyn std::error::Error>> {
        match &envelope.payload {
            Some(TunnelEnvelopePayload::HttpRequest(req)) => {
                // 根据规则路由到目标 client
                if let Some(target_client) = self.route_http_request(req).await {
                    if let Some(tx) = self.server_manager.get_tx_by_client(&target_client).await {
                        tx.send(envelope).await?;
                    }
                }
            }
            Some(TunnelEnvelopePayload::HttpResponse(resp)) => {
                // 处理 HTTP 响应
                self.handle_http_response(envelope).await?;
            }
            _ => {}
        }
        Ok(())
    }

    // gRPC 处理
    async fn handle_grpc(&self, envelope: TunnelEnvelope) -> Result<(), Box<dyn std::error::Error>> {
        match &envelope.payload {
            Some(TunnelEnvelopePayload::GrpcFrame(frame)) => {
                // 根据 service/method 路由
                if let Some(target_client) = self.route_grpc_request(frame).await {
                    if let Some(tx) = self.server_manager.get_tx_by_client(&target_client).await {
                        tx.send(envelope).await?;
                    }
                }
            }
            _ => {}
        }
        Ok(())
    }

    // WebSocket 处理
    async fn handle_websocket(&self, envelope: TunnelEnvelope) -> Result<(), Box<dyn std::error::Error>> {
        match &envelope.payload {
            Some(TunnelEnvelopePayload::WsFrame(frame)) => {
                // 根据 session_id 路由
                if let Some(target_client) = self.route_websocket_frame(&envelope.session_id).await {
                    if let Some(tx) = self.server_manager.get_tx_by_client(&target_client).await {
                        tx.send(envelope).await?;
                    }
                }
            }
            _ => {}
        }
        Ok(())
    }
}
```
6. 连接生命周期管理
6.1 自动重连机制

```
pub struct TunnelConnection {
    stream_id: String,
    stream_name: String,
    tx: mpsc::Sender<TunnelMessage>,
    rx: mpsc::Receiver<TunnelMessage>,
    manager: Arc<TunnelConnectionManager>,
}

impl TunnelConnection {
    // 启动连接监听
    pub async fn start(&mut self) {
        while let Some(msg) = self.rx.recv().await {
            // 处理消息
            if let Err(e) = self.handle_message(msg).await {
                // 连接异常，触发重连
                self.reconnect().await;
                break;
            }
        }
    }

    // 重连逻辑
    async fn reconnect(&self) {
        let mut backoff = ExponentialBackoff::default();
        
        loop {
            match self.establish_connection().await {
                Ok((new_tx, new_rx)) => {
                    // 重新注册到 manager
                    let stream_info = StreamInfo {
                        stream_id: self.stream_id.clone(),
                        stream_name: self.stream_name.clone(),
                        tx: new_tx,
                        client_id: None,
                        group_id: None,
                    };
                    self.manager.register_stream(stream_info).await;
                    break;
                }
                Err(_) => {
                    if let Some(delay) = backoff.next_backoff() {
                        tokio::time::sleep(delay).await;
                    } else {
                        break;
                    }
                }
            }
        }
    }
}
```
7. 总结
采用统一的 TunnelConnectionManager 对象管理所有 stream 的 tx/rx，便于多协议多 stream 的高效路由和生命周期管理。
server 端通过额外的映射（如 client_id → stream_id、client_group → Vec<client_id>）实现对多 client group、多 client 的灵活管理和负载均衡。
proto 层通过 stream_id、stream_name 字段为未来多 stream/多连接/多协议扩展打下基础。
handler 只需通过 manager 查找目标 tx，发送消息即可，极大简化了业务逻辑。
自动重连、健康检查等高可用机制可以在 manager/connection 层统一实现，业务层无感知。



stream name/id 与 channel/stream 的关联
你在管理层（如 TunnelConnectionManager）里，维护了 stream name/id 到 channel/stream/tx 的映射关系。

```
  // Rust伪代码
  struct StreamInfo {
      stream_name: String,
      session_id: String,
      channel_id: String, // 或 channel/connection 的唯一标识
      tx: mpsc::Sender<TunnelMessage>,
      // 其他元数据
  }
  // manager: stream_name/id -> StreamInfo
```
2. channel/stream 断开时的感知
gRPC/tonic：当 channel 断开或 stream 关闭时，相关的 rx/tx 会返回错误（如 Err(Status)），你可以在 handler 或管理器里捕获到。
检测方式：
读写 stream 时遇到 Err(Status)，说明该 stream/channel 已不可用。
tonic 的 channel 支持健康检查（如 .ready().await），可以主动检测 channel 状态。
3. 自动 cancel 和重建流程
步骤一：感知断开
在每个 stream 的 handler/task 里，监听 rx/tx 的错误返回。
一旦发现错误（如 Status::unavailable()、Status::cancelled()），立即触发清理和重建逻辑。
步骤二：清理关联
在 TunnelConnectionManager 里，将该 channel/stream 关联的所有 stream name/id 标记为“不可用”或直接移除。
通知上层 handler 取消相关 session。
步骤三：自动重建
启动重连/重建逻辑（可用 backoff、定时重试等）。
新建 channel/stream 后，重新注册 stream name/id 到新的 tx/rx。
恢复业务流量。
4. Rust 伪代码示例
// 1. 维护映射
struct TunnelConnectionManager {
    // stream_name -> StreamInfo
    streams: DashMap<String, StreamInfo>,
    // channel_id -> Vec<stream_name>
    channel_streams: DashMap<String, Vec<String>>,
}

// 2. 断开时清理
fn on_channel_closed(&self, channel_id: &str) {
    if let Some(stream_names) = self.channel_streams.remove(channel_id) {
        for stream_name in stream_names {
            self.streams.remove(&stream_name);
            // 通知 handler/session cancel
        }
    }
}

// 3. handler 里检测
async fn handle_stream(stream_info: StreamInfo, manager: Arc<TunnelConnectionManager>) {
    while let Some(msg) = stream_info.rx.recv().await {
        // ...
    }
    // rx关闭，说明stream/channel挂了
    manager.on_channel_closed(&stream_info.channel_id);
    // 触发重建
}

自动重建的最佳实践
重连/重建：用 exponential backoff，避免频繁重试。
幂等注册：新建 stream/channel 后，重新注册 stream name/id 到 manager。
业务透明：上层 handler 只需通过 manager 获取可用的 tx，manager 自动屏蔽重建细节。
6. 总结
通过 manager 维护 stream name/id 到 channel/stream/tx 的映射。
channel/stream 断开时，捕获异常，自动清理和 cancel 相关 session。
自动重建 channel/stream 并重新注册映射，业务层无感知。
这样可以实现高可用、自动恢复的 tunnel 通信。